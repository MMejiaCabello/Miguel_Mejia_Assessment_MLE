# 04_monitoring_eval ‚Äî Evaluaci√≥n y Monitoreo del Modelo

Este README documenta por completo el **Punto 4: Evaluaci√≥n y Monitoreo del Modelo** del *Assessment T√©cnico ‚Äì MLE*. Aqu√≠ se describe y se **simula** un sistema de monitoreo con m√©tricas, alertas, **test A/B** y **rollback**. Todo est√° listo para ejecutarse localmente y dejarlo orquestado (Cloud Run Job / Composer / Scheduler) si lo deseas.

> **Repositorio / Carpeta:** `04_monitoring_eval/`

---

## üéØ Objetivos
1. **Medir desempe√±o** del modelo de churn (AUC, F1, precisi√≥n, etc.).
2. **Detectar drift de datos** mediante **PSI** y gatillar alertas.
3. **Simular A/B testing** para comparar versiones de modelo.
4. **Definir pol√≠tica de rollback** (y ejemplo de split de tr√°fico en Vertex AI).

---

## üì¶ Contenido de la carpeta
```
04_monitoring_eval/
‚îú‚îÄ README.md                       ‚Üê Este documento
‚îú‚îÄ monitoring_notebook.ipynb       ‚Üê Demo completa (m√©tricas, PSI, alertas, A/B, rollback)
‚îú‚îÄ monitor_metrics.py              ‚Üê CLI para monitoreo por lotes (programable)
‚îú‚îÄ drift_utils.py                  ‚Üê Utilidades de drift (PSI y reporte)
‚îú‚îÄ ab_test_sim.py                  ‚Üê Utilidad para evaluar A/B y elegir ganador
‚îî‚îÄ baseline_metrics.json           ‚Üê Baseline de m√©tricas para comparaciones
```

---

## üßÆ M√©tricas monitoreadas
**Entrenamiento/Validaci√≥n**
- `accuracy`, `precision`, `recall`, `f1`, `roc_auc` (y `ks` opcional).

**Producci√≥n / Post-despliegue (simulado aqu√≠)**
- Degradaci√≥n de `roc_auc` vs. `baseline`.
- **Drift** de entrada con **PSI** (Population Stability Index).
- (Extensible) Latencia de predicci√≥n, tasas de error, throughput, etc.

> En `monitor_metrics.py` las m√©tricas se calculan sobre un batch actual (real o simulado) y se comparan contra un **baseline** previamente generado.

---

## üî≠ Detecci√≥n de drift con PSI
- **PSI** mide el cambio de distribuci√≥n entre un conjunto **de referencia** (baseline) y uno **actual**.
- Interpretaci√≥n t√≠pica:
  - PSI < 0.10 ‚Üí **No significativo**
  - 0.10 ‚â§ PSI ‚â§ 0.25 ‚Üí **Moderado**
  - PSI > 0.25 ‚Üí **Severo**

**C√°lculo (a alto nivel):**
1. Discretizar la variable en *bins* (cuantiles robustos).
2. Calcular proporci√≥n por bin en **baseline** y **actual**.
3. Sumar \( (p_{act} - p_{base}) \cdot \ln\frac{p_{act}}{p_{base}} \) en todos los bins.

`drift_utils.py` implementa `population_stability_index()` y `drift_report()` para varias features.

---

## üö¶ Umbrales y reglas de alerta (editables)
En `monitor_metrics.py`:
- `auc_min = 0.75` ‚Üí si el **ROC AUC** actual es menor, **alerta**.
- `auc_drop_pct_warn = 0.05` ‚Üí si el AUC cae **‚â•5%** vs. baseline, **alerta**.
- `psi_warn = 0.10`, `psi_severe = 0.25` ‚Üí reporta **drift moderado** y **severo** por feature.

Puedes sobreescribirlos con `--thresholds-json path.json`.

---

## ‚öôÔ∏è Requisitos
- Python 3.9+ (recomendado)
- `pandas`, `numpy`, `scikit-learn`, `matplotlib`
- Dataset `clientes.csv` (local o desde GCS en notebook usando `gcsfs`)

> El notebook ya demuestra c√≥mo cargar `clientes.csv` y correr todo el flujo.

---

## üõ†Ô∏è Quickstart (local)
### 1) Crear baseline
Genera (o regenera) el baseline a partir de `clientes.csv`:
```bash
python 04_monitoring_eval/monitor_metrics.py \
  --make-baseline \
  --data-path clientes.csv
```
Esto produce/actualiza `baseline_metrics.json` con: features, m√©tricas y metadatos.

### 2) Ejecutar monitoreo (con drift simulado)
```bash
python 04_monitoring_eval/monitor_metrics.py \
  --data-path clientes.csv \
  --simulate-drift
```
**Salida:**
- Imprime un **reporte JSON** con m√©tricas actuales, ca√≠da relativa de AUC y PSI por feature.
- Escribe `monitor_report.json`.
- **Exit code:** `0` si no hay alertas, `1` si hay alertas ‚Üí √∫til para orquestadores/alerting.

### 3) Notebook de demo
Abre `monitoring_notebook.ipynb` para ejecutar paso a paso:
- C√°lculo de m√©tricas
- PSI + gr√°fico
- Reglas de alerta
- Simulaci√≥n **A/B** y selecci√≥n de ganador
- Pol√≠tica de **rollback**

---

## üì§ Formato de salida (monitor_report.json)
Ejemplo de estructura:
```json
{
  "metrics_current": {"accuracy": 0.82, "precision": 0.64, "recall": 0.59, "f1": 0.61, "roc_auc": 0.78},
  "metrics_baseline": {"accuracy": 0.84, "precision": 0.66, "recall": 0.60, "f1": 0.63, "roc_auc": 0.82},
  "auc_drop_pct": 0.0487,
  "psi": {"age": 0.12, "total_purchases": 0.07, "avg_purchase_value": 0.18},
  "alerts": ["Ca√≠da relativa de AUC 5.1% ‚â• 5%"],
  "generated_at": "2025-08-11T07:00:00Z"
}
```

---

## üîî Alertas (simuladas)
- El **exit code** distinto de cero permite disparar alertas desde el orquestador.
- Puedes a√±adir un *hook* HTTP/Slack/Email. Ejemplo (pseudoc√≥digo Python):
```python
import json, os, requests
webhook = os.getenv("SLACK_WEBHOOK_URL")
rep = json.load(open("monitor_report.json"))
if rep["alerts"] and webhook:
    txt = "\n".join(["*ALERTAS de Monitoreo*", *rep["alerts"]])
    requests.post(webhook, json={"text": txt})
```

> En producci√≥n, centraliza logs en **Cloud Logging**, usa **Error Reporting** y notificaciones a **Slack/Email**.

---

## üÜö Test A/B (simulaci√≥n)
`ab_test_sim.py` ofrece:
- `evaluate_ab(df)` ‚Üí calcula `accuracy`, `f1`, `roc_auc` para `v1` y `v2`.
- `pick_winner(metrics_dict, primary="roc_auc", secondary="f1")` ‚Üí decide ganador.

**Flujo t√≠pico:**
1. Redirige un **% de tr√°fico** a `v2` (candidato).
2. Almacena predicciones (BigQuery) etiquetadas por variante.
3. Ejecuta `evaluate_ab` + `pick_winner` sobre la cohorte A/B.
4. Si `v2` ‚â• `v1` en AUC y no peor en F1 ‚Üí **promueve** `v2`.

---

## üîÅ Rollback & Split de Tr√°fico (Vertex AI)
**Pol√≠tica sugerida:**
- Rollback inmediato si:
  - `roc_auc` cae **‚â• 10%** vs. baseline **o**
  - PSI **severo** en **‚â• 2** features cr√≠ticas.

**Ejemplo (Python, Vertex AI):**
```python
from google.cloud import aiplatform

project = "<PROJECT_ID>"
location = "us-central1"
endpoint_name = "projects/<PROJECT_ID>/locations/us-central1/endpoints/<ENDPOINT_ID>"

aiplatform.init(project=project, location=location)
endpoint = aiplatform.Endpoint(endpoint_name)

# Supongamos que el endpoint tiene despliegues: {"v1": 90, "v2": 10}
# Hacer rollback total a v1:
endpoint.update_traffic_split({"v1": 100})

# O hacer A/B 80/20:
endpoint.update_traffic_split({"v1": 80, "v2": 20})
```

**Ejemplo (gcloud):**
```bash
gcloud ai endpoints update-traffic-split \
  --project=<PROJECT_ID> \
  --location=us-central1 \
  <ENDPOINT_ID> \
  --traffic-split=\
"v1=90,v2=10"
```

> Mant√©n m√∫ltiples **Model Versions** registradas. Documenta claramente cu√°l es la *last known good* para revertir r√°pido.

---

## üóÑÔ∏è Persistencia y tableros (opcional recomendado)
- **BigQuery** como *feature store* liviano de predicciones:
  - Tabla `predictions_log(model_version STRING, inference_ts TIMESTAMP, y_proba FLOAT64, y_pred INT64, customer_id STRING, ‚Ä¶)`
  - Tabla `ground_truth(customer_id STRING, y_true INT64, labeled_ts TIMESTAMP, ‚Ä¶)`
- **Dashboards**: Looker Studio / Grafana sobre BigQuery para: AUC rolling, F1, PSI top-N, latencia, tasa de errores.

**Ejemplo de consulta de AUC diario (esbozo):**
```sql
SELECT
  DATE(inference_ts) AS day,
  APPROX_QUANTILES(y_proba, 100)[OFFSET(50)] AS median_score,
  COUNT(*) AS n
FROM dataset.predictions_log
GROUP BY day
ORDER BY day DESC;
```

---

## üß∞ Orquestaci√≥n (opciones)
- **Cloud Run Job**: empaqueta `monitor_metrics.py` y dependencias en un contenedor.
- **Cloud Scheduler**: dispara el Job cada N minutos/horas.
- **Cloud Composer (Airflow)**: DAG con tareas de verificaci√≥n, c√°lculo PSI y env√≠o de alertas.

**Sugerencia de variables de entorno**
- `DATA_PATH` ‚Üí ruta al batch actual (GCS/BigQuery extra√≠do a CSV/Parquet).
- `THRESHOLDS_JSON` ‚Üí archivo con umbrales custom.
- `SLACK_WEBHOOK_URL` ‚Üí URL de webhook para notificaciones.

---

## üß™ Validaci√≥n de fin a fin
1. Genera `baseline_metrics.json`.
2. Ejecuta monitoreo con `--simulate-drift` y verifica:
   - `monitor_report.json` contenga PSI > 0.10 en alguna feature.
   - `alerts` no vac√≠o.
   - Exit code = 1.
3. Ejecuta el notebook y confirma la simulaci√≥n **A/B** y el bloque de **rollback**.

---

## ‚ùì FAQ
**¬øDe d√≥nde salen los umbrales?**  
Parten de recomendaciones comunes (AUC ‚â• 0.75 y PSI). Ajusta seg√∫n tu dominio.

**¬øSe puede integrar con Vertex Model Monitoring?**  
S√≠. Este paquete es una simulaci√≥n ligera y portable. Puedes migrar PSI y reglas a Vertex o a Grafana/Prometheus.

**¬øC√≥mo guardo el reporte en BigQuery?**  
Parsea `monitor_report.json` y haz un `INSERT` a una tabla de auditor√≠a.

---

## üöß Troubleshooting
- **"No existe baseline_metrics.json"** ‚Üí ejecuta `--make-baseline` primero.
- **PSI NaN** ‚Üí puede ser por baja varianza o columnas no num√©ricas; revisa bins.
- **AUC baja siempre** ‚Üí revisa *data leakage*, balanceo de clases, o umbral de clasificaci√≥n (0.5).

---

## ‚úÖ Checklist de cumplimiento del Assessment
- **M√©tricas:** accuracy, precision, recall, f1, roc_auc.
- **Alertas:** por AUC m√≠nimo, ca√≠da relativa y PSI (moderado/severo).
- **A/B testing:** funci√≥n para evaluar v1 vs v2 y criterio de ganador.
- **Rollback:** pol√≠tica definida + ejemplos de **traffic split** (Vertex AI).
- **Documentaci√≥n:** este README + notebook demostrativo.

---

## üìö Referencias sugeridas
- Monitoreo de modelos: drift, data quality y concept drift (literatura MLOps general).
- PSI y KS en riesgo crediticio (metodolog√≠a cl√°sica, aplicable a churn con precauci√≥n).
- Documentaci√≥n de Vertex AI Endpoints para **traffic split** y **model versions**.
