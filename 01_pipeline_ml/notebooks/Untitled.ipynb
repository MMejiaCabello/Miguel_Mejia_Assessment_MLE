{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b8ccd-3bdf-4c48-a388-7cbab1a7f0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aef383a-1d13-4f40-939b-318d2bc35f90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbformat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Try to import nbformat to build a notebook programmatically\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnbformat\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnbformat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m new_notebook, new_markdown_cell, new_code_cell\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nbformat'"
     ]
    }
   ],
   "source": [
    "# This script creates a Jupyter notebook for data ingestion and preprocessing\n",
    "# tailored to the \"primera etapa\" of the Assessment MLE. It also previews the\n",
    "# provided CSV so you can confirm the schema at a glance.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import nbformat to build a notebook programmatically\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "\n",
    "# Paths\n",
    "base_dir = \"/mnt/data\"\n",
    "notebook_path = os.path.join(base_dir, \"01_pipeline_ml\", \"ingesta_preprocesamiento.ipynb\")\n",
    "os.makedirs(os.path.dirname(notebook_path), exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "csv_path = \"/mnt/data/clientes.csv\"\n",
    "parquet_output_dir = \"/mnt/data/01_pipeline_ml/outputs\"\n",
    "os.makedirs(parquet_output_dir, exist_ok=True)\n",
    "\n",
    "# Build notebook cells\n",
    "cells = []\n",
    "\n",
    "# 1) Title & overview\n",
    "cells.append(new_markdown_cell(\"\"\"\n",
    "# Ingesta y Preprocesamiento (Primera Etapa)\n",
    "\n",
    "**Objetivo:** Cargar `clientes.csv`, validar esquema, limpiar y estandarizar tipos, crear *features* iniciales y preparar un dataset **curado** listo para *feature engineering* y *modelado* en etapas posteriores.\n",
    "\n",
    "**Contenido:**\n",
    "1. Configuración y dependencias\n",
    "2. Ingesta del CSV y validación de esquema\n",
    "3. Limpieza: tipos, valores faltantes y reglas de negocio\n",
    "4. *EDA* breve (sanity checks)\n",
    "5. *Features* iniciales (*recencia*, *tenure*, *frequency* proxy)\n",
    "6. Preprocesamiento con `sklearn` (`ColumnTransformer` + `Pipeline`)\n",
    "7. Exportación de artefactos\n",
    "\"\"\"))\n",
    "\n",
    "# 2) Install/imports (kept minimal; most envs already have these)\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "# === 1. Configuración y dependencias ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "DATA_PATH = \"{csv_path}\"\n",
    "OUTPUT_DIR = \"{parquet_output_dir}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"[INFO] DATA_PATH: {csv_path}\")\n",
    "print(f\"[INFO] OUTPUT_DIR: {parquet_output_dir}\")\n",
    "\"\"\".format(csv_path=csv_path, parquet_output_dir=parquet_output_dir)))\n",
    "\n",
    "# 3) Load CSV & schema\n",
    "cells.append(new_markdown_cell(\"## 2. Ingesta del CSV y validación de esquema\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "expected_cols = [\n",
    "    \"customer_id\", \"age\", \"gender\", \"signup_date\", \"last_purchase_date\",\n",
    "    \"total_purchases\", \"avg_purchase_value\", \"is_active\", \"churned\"\n",
    "]\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"[INFO] Columnas encontradas:\", list(df_raw.columns))\n",
    "missing = set(expected_cols) - set(df_raw.columns)\n",
    "extra = set(df_raw.columns) - set(expected_cols)\n",
    "if missing:\n",
    "    print(f\"[WARN] Faltan columnas: {missing}\")\n",
    "if extra:\n",
    "    print(f\"[WARN] Columnas adicionales: {extra}\")\n",
    "\n",
    "df_raw.head(3)\n",
    "\"\"\"))\n",
    "\n",
    "# 4) Cast dtypes & basic cleaning\n",
    "cells.append(new_markdown_cell(\"## 3. Limpieza: tipos, valores faltantes y reglas de negocio\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "# Tipos de datos: aseguramos fechas y numéricos\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Fechas\n",
    "for col in [\"signup_date\", \"last_purchase_date\"]:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# Numéricos\n",
    "numeric_int = [\"age\", \"total_purchases\", \"is_active\", \"churned\"]\n",
    "for col in numeric_int:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "df[\"avg_purchase_value\"] = pd.to_numeric(df[\"avg_purchase_value\"], errors=\"coerce\")\n",
    "\n",
    "# Valores imposibles / reglas de negocio simples\n",
    "# - Edad en [18, 100] (dejamos margen por datos sucios)\n",
    "df.loc[~df[\"age\"].between(18, 100), \"age\"] = pd.NA\n",
    "\n",
    "# - total_purchases no negativo\n",
    "df.loc[df[\"total_purchases\"].fillna(-1) < 0, \"total_purchases\"] = pd.NA\n",
    "\n",
    "# - avg_purchase_value positivo\n",
    "df.loc[df[\"avg_purchase_value\"].fillna(-1) <= 0, \"avg_purchase_value\"] = pd.NA\n",
    "\n",
    "# - last_purchase_date no debe ser antes que signup_date (si lo es, marcamos como NaT)\n",
    "mask_bad_dates = (df[\"last_purchase_date\"].notna() & df[\"signup_date\"].notna() &\n",
    "                  (df[\"last_purchase_date\"] < df[\"signup_date\"]))\n",
    "df.loc[mask_bad_dates, \"last_purchase_date\"] = pd.NaT\n",
    "\n",
    "df.head(3)\n",
    "\"\"\"))\n",
    "\n",
    "# 5) Brief EDA\n",
    "cells.append(new_markdown_cell(\"## 4. EDA breve (sanity checks)\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "print(\"[INFO] Dimensiones:\", df.shape)\n",
    "print(\"\\\\n[INFO] Porcentaje de nulos por columna:\")\n",
    "print((df.isna().mean() * 100).round(2).sort_values(ascending=False))\n",
    "\n",
    "print(\"\\\\n[INFO] Balance de 'churned':\")\n",
    "print(df[\"churned\"].value_counts(dropna=False, normalize=True).round(3))\n",
    "\n",
    "print(\"\\\\n[INFO] Rango de fechas:\")\n",
    "for c in [\"signup_date\", \"last_purchase_date\"]:\n",
    "    print(c, \"->\", df[c].min(), \"—\", df[c].max())\n",
    "\"\"\"))\n",
    "\n",
    "# 6) Feature creation\n",
    "cells.append(new_markdown_cell(\"## 5. *Features* iniciales\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "# Elegimos una 'fecha de referencia' como el máximo 'last_purchase_date' disponible\n",
    "# (alternativamente, usar 'hoy' si el negocio lo requiere)\n",
    "ref_date = pd.to_datetime(df[\"last_purchase_date\"].max())\n",
    "\n",
    "df[\"tenure_days\"] = (ref_date - df[\"signup_date\"]).dt.days\n",
    "df[\"recency_days\"] = (ref_date - df[\"last_purchase_date\"]).dt.days\n",
    "\n",
    "# Frecuencia proxy: total_purchases / (tenure en meses aprox.)\n",
    "df[\"tenure_months\"] = df[\"tenure_days\"] / 30.0\n",
    "df[\"frequency_pm\"] = df[\"total_purchases\"] / df[\"tenure_months\"]\n",
    "df.loc[~np.isfinite(df[\"frequency_pm\"]), \"frequency_pm\"] = np.nan  # protege divisiones por 0\n",
    "\n",
    "df[[\"customer_id\",\"tenure_days\",\"recency_days\",\"frequency_pm\"]].head(3)\n",
    "\"\"\"))\n",
    "\n",
    "# 7) Preprocessing with sklearn\n",
    "cells.append(new_markdown_cell(\"## 6. Preprocesamiento con `sklearn`\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "# Definimos variables\n",
    "target_col = \"churned\"\n",
    "id_cols = [\"customer_id\"]\n",
    "date_cols = [\"signup_date\", \"last_purchase_date\"]\n",
    "num_cols = [\n",
    "    \"age\", \"total_purchases\", \"avg_purchase_value\",\n",
    "    \"is_active\", \"tenure_days\", \"recency_days\", \"frequency_pm\"\n",
    "]\n",
    "cat_cols = [\"gender\"]\n",
    "\n",
    "# Dataset para modelado (excluye IDs y fechas crudas)\n",
    "model_df = df[id_cols + date_cols + num_cols + cat_cols + [target_col]].copy()\n",
    "\n",
    "# Separación X/y (guardamos id para trazabilidad si hace falta)\n",
    "X = model_df[num_cols + cat_cols]\n",
    "y = model_df[target_col].astype(\"Int64\")\n",
    "\n",
    "# Transformadores\n",
    "num_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, num_cols),\n",
    "        (\"cat\", cat_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# *Fit* del preprocesador (sin entrenar modelo todavía)\n",
    "X_pre = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"[INFO] Forma de X antes:\", X.shape)\n",
    "print(\"[INFO] Forma de X después del preprocesamiento:\", X_pre.shape)\n",
    "\"\"\"))\n",
    "\n",
    "# 8) Train/val split ready (optional)\n",
    "cells.append(new_markdown_cell(\"### (Opcional) Train/Validation Split\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"[INFO] X_train:\", X_train.shape, \"| X_valid:\", X_valid.shape)\n",
    "\"\"\"))\n",
    "\n",
    "# 9) Export artifacts\n",
    "cells.append(new_markdown_cell(\"## 7. Exportación de artefactos\"))\n",
    "cells.append(new_code_cell(\"\"\"\n",
    "# %% [code]\n",
    "# Guardamos dataset curado (parquet) y el preprocesador\n",
    "curated_path = os.path.join(OUTPUT_DIR, \"clientes_curado.parquet\")\n",
    "preproc_path = os.path.join(OUTPUT_DIR, \"preprocessor.joblib\")\n",
    "\n",
    "df.to_parquet(curated_path, index=False)\n",
    "joblib.dump(preprocessor, preproc_path)\n",
    "\n",
    "print(f\"[OK] Curado guardado en: {curated_path}\")\n",
    "print(f\"[OK] Preprocessor guardado en: {preproc_path}\")\n",
    "\n",
    "# (Opcional) Exportación de particiones de train/valid para etapas siguientes\n",
    "X_train_path = os.path.join(OUTPUT_DIR, \"X_train.parquet\")\n",
    "X_valid_path = os.path.join(OUTPUT_DIR, \"X_valid.parquet\")\n",
    "y_train_path = os.path.join(OUTPUT_DIR, \"y_train.parquet\")\n",
    "y_valid_path = os.path.join(OUTPUT_DIR, \"y_valid.parquet\")\n",
    "\n",
    "X_train.to_parquet(X_train_path, index=False)\n",
    "X_valid.to_parquet(X_valid_path, index=False)\n",
    "pd.DataFrame({\"churned\": y_train}).to_parquet(y_train_path, index=False)\n",
    "pd.DataFrame({\"churned\": y_valid}).to_parquet(y_valid_path, index=False)\n",
    "\n",
    "print(\"[OK] Particiones de train/valid exportadas.\")\n",
    "\"\"\"))\n",
    "\n",
    "# 10) Notes for GCS/Vertex\n",
    "cells.append(new_markdown_cell(\"\"\"\n",
    "> **Nota (GCP):**  \n",
    "> - Para esta primera etapa trabajamos localmente.  \n",
    "> - En producción, estos artefactos se escriben a **GCS** (`gs://...`) y el *preprocessor* pasa al *model training job* en Vertex AI.  \n",
    "> - Este notebook puede convertirse en un *Python component* dentro de un *Vertex Pipeline* o bien en un *Matillion Python Script* si orquestas fuera de Vertex.\n",
    "\"\"\"))\n",
    "\n",
    "# Create the notebook\n",
    "nb = new_notebook(cells=cells, metadata={\n",
    "    \"kernelspec\": {\"name\": \"python3\", \"display_name\": \"Python 3\"},\n",
    "    \"language_info\": {\"name\": \"python\", \"version\": \"3.x\"},\n",
    "    \"authors\": [{\"name\": \"Assessment MLE - Ingesta/Preprocesamiento\"}],\n",
    "    \"created\": datetime.utcnow().isoformat() + \"Z\"\n",
    "})\n",
    "\n",
    "with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "notebook_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42007a9a-e499-450e-b78f-911af827ac27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-langchain-agent-env-conda_environment_name",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "langchain-agent (Local)",
   "language": "python",
   "name": "conda-env-langchain-agent-env-conda_environment_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
